{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchfly.modules.transformers import (GPT2SimpleLM,\n",
    "                                           UnifiedGPT2SmallConfig,\n",
    "                                           UnifiedGPT2MediumConfig,\n",
    "                                           UnifiedGPT2LargeConfig,\n",
    "                                           UnifiedGPT2DistillConfig,\n",
    "                                           UnifiedGPT2XLConfig)\n",
    "                                           \n",
    "from torchfly.text.tokenizers import UnifiedBPETokenizer\n",
    "from torchfly.utils import get_pretrained_states\n",
    "from transformers import GPT2LMHeadModel, RobertaTokenizer, GPT2Tokenizer, RobertaModel\n",
    "\n",
    "from torchfly.text.decode import top_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: /home/wuqy1203/.cache/torchfly/models/unified-gpt2-large.pth\n"
     ]
    }
   ],
   "source": [
    "states = get_pretrained_states(\"unified-gpt2-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2SimpleLM(UnifiedGPT2LargeConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['lm_head.decoder.weight'], unexpected_keys=['transformer.h.0.attn.bias', 'transformer.h.1.attn.bias', 'transformer.h.2.attn.bias', 'transformer.h.3.attn.bias', 'transformer.h.4.attn.bias', 'transformer.h.5.attn.bias', 'transformer.h.6.attn.bias', 'transformer.h.7.attn.bias', 'transformer.h.8.attn.bias', 'transformer.h.9.attn.bias', 'transformer.h.10.attn.bias', 'transformer.h.11.attn.bias', 'transformer.h.12.attn.bias', 'transformer.h.13.attn.bias', 'transformer.h.14.attn.bias', 'transformer.h.15.attn.bias', 'transformer.h.16.attn.bias', 'transformer.h.17.attn.bias', 'transformer.h.18.attn.bias', 'transformer.h.19.attn.bias', 'transformer.h.20.attn.bias', 'transformer.h.21.attn.bias', 'transformer.h.22.attn.bias', 'transformer.h.23.attn.bias', 'transformer.h.24.attn.bias', 'transformer.h.25.attn.bias', 'transformer.h.26.attn.bias', 'transformer.h.27.attn.bias', 'transformer.h.28.attn.bias', 'transformer.h.29.attn.bias', 'transformer.h.30.attn.bias', 'transformer.h.31.attn.bias', 'transformer.h.32.attn.bias', 'transformer.h.33.attn.bias', 'transformer.h.34.attn.bias', 'transformer.h.35.attn.bias', 'lm_head.weight'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(states, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0149, -0.0209,  0.0021,  ...,  0.0336, -0.0005, -0.0090],\n",
       "        [ 0.0055, -0.0438,  0.0013,  ...,  0.0671,  0.0329, -0.0399],\n",
       "        [ 0.0585,  0.0603,  0.0302,  ..., -0.1041, -0.0566, -0.0330],\n",
       "        ...,\n",
       "        [-0.0209,  0.0443,  0.0327,  ..., -0.0081, -0.0022,  0.0123],\n",
       "        [ 0.0028,  0.0243,  0.0061,  ..., -0.0177, -0.0040,  0.0213],\n",
       "        [-0.0049,  0.0257,  0.0111,  ..., -0.0429,  0.0170,  0.0346]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.decoder.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"roberta-large.pth\")\n",
    "torch.save(model.half().state_dict(), \"roberta-large-fp16.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', '<pad>', '<unk>', '<mask>']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta_tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_interpolate(x, vocab_size):\n",
    "    return x[np.random.randint(vocab_size, size=20), :].mean(0) + \\\n",
    "            torch.randn(x.shape[-1]) * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "gpt2_small = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_special_tokens = 8\n",
    "\n",
    "# copy the original embedding\n",
    "new_embedding = nn.Embedding(gpt2_small.config.vocab_size + num_special_tokens, gpt2_small.config.n_embd)\n",
    "new_embedding.weight.data[:gpt2_small.config.vocab_size, :] = gpt2_small.transformer.wte.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the first three, use random interpolate\n",
    "for i in range(num_special_tokens):\n",
    "    new_embedding.weight.data[gpt2_small.config.vocab_size+i, :] = \\\n",
    "        random_interpolate(new_embedding.weight.data, gpt2_small.config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tied\n",
    "gpt2_small.transformer.wte = new_embedding\n",
    "gpt2_small.lm_head.weight = gpt2_small.transformer.wte.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get special tokens\n",
    "special_tokens = set(roberta_tokenizer.encoder.keys()).difference(set(gpt_tokenizer.encoder.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine the mapping\n",
    "r2g_mapping = []\n",
    "special_start_count = 50257\n",
    "\n",
    "for r_idx in range(50265):\n",
    "    r_token = roberta_tokenizer.decoder[r_idx]\n",
    "    \n",
    "    if r_token in special_tokens:\n",
    "        r2g_mapping.append(special_start_count)\n",
    "        special_start_count += 1\n",
    "    else:\n",
    "        g_idx = gpt_tokenizer.encoder[r_token]\n",
    "        r2g_mapping.append(g_idx)\n",
    "        \n",
    "# r2g = {v:k for k,v in enumerate(r2g_mapping)}\n",
    "# r2g_mapping = [r2g[i] for i in range(50265)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rearrange the weight\n",
    "gpt2_small_states = gpt2_small.state_dict()\n",
    "gpt2_small_states = {k: v for k, v in gpt2_small_states.items() if '.attn.bias' not in k}\n",
    "# gpt2_small_states['transformer.wte.weight'] = gpt2_small_states['transformer.wte.weight'][r2g_mapping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get special tokens\n",
    "special_tokens = set(roberta_tokenizer.encoder.keys()).difference(set(gpt_tokenizer.encoder.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "num_special_tokens = 8\n",
    "\n",
    "# copy the original embedding\n",
    "new_embedding = nn.Embedding(gpt2_model.config.vocab_size + num_special_tokens, gpt2_model.config.n_embd)\n",
    "new_embedding.weight.data[:gpt2_model.config.vocab_size, :] = gpt2_model.transformer.wte.weight.data\n",
    "\n",
    "# for the first three, use random interpolate\n",
    "for i in range(num_special_tokens):\n",
    "    new_embedding.weight.data[gpt2_model.config.vocab_size+i, :] = \\\n",
    "        random_interpolate(new_embedding.weight.data, gpt2_model.config.vocab_size)\n",
    "\n",
    "# set tied\n",
    "gpt2_model.transformer.wte = new_embedding\n",
    "gpt2_model.lm_head.weight = gpt2_model.transformer.wte.weight\n",
    "\n",
    "# redefine the mapping\n",
    "r2g_mapping = []\n",
    "special_start_count = 50257\n",
    "\n",
    "for r_idx in range(50265):\n",
    "    r_token = roberta_tokenizer.decoder[r_idx]\n",
    "    \n",
    "    if r_token in special_tokens:\n",
    "        r2g_mapping.append(special_start_count)\n",
    "        special_start_count += 1\n",
    "    else:\n",
    "        g_idx = gpt_tokenizer.encoder[r_token]\n",
    "        r2g_mapping.append(g_idx)\n",
    "\n",
    "# rearrange the weight\n",
    "gpt2_model_states = gpt2_model.state_dict()\n",
    "gpt2_model_states = {k: v for k, v in gpt2_model_states.items() if '.attn.bias' not in k}\n",
    "gpt2_model_states['transformer.wte.weight'] = gpt2_model_states['transformer.wte.weight'][r2g_mapping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gpt2_model.state_dict(), \"unified_gpt2_medium.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gpt2_model.half().state_dict(), \"unified_gpt2_medium_fp16.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\")\n",
    "\n",
    "num_special_tokens = 8\n",
    "\n",
    "# copy the original embedding\n",
    "new_embedding = nn.Embedding(gpt2_model.config.vocab_size + num_special_tokens, gpt2_model.config.n_embd)\n",
    "new_embedding.weight.data[:gpt2_model.config.vocab_size, :] = gpt2_model.transformer.wte.weight.data\n",
    "\n",
    "# for the first three, use random interpolate\n",
    "for i in range(num_special_tokens):\n",
    "    new_embedding.weight.data[gpt2_model.config.vocab_size+i, :] = \\\n",
    "        random_interpolate(new_embedding.weight.data, gpt2_model.config.vocab_size)\n",
    "\n",
    "# set tied\n",
    "gpt2_model.transformer.wte = new_embedding\n",
    "gpt2_model.lm_head.weight = gpt2_model.transformer.wte.weight\n",
    "\n",
    "# redefine the mapping\n",
    "r2g_mapping = []\n",
    "special_start_count = 50257\n",
    "\n",
    "for r_idx in range(50265):\n",
    "    r_token = roberta_tokenizer.decoder[r_idx]\n",
    "    \n",
    "    if r_token in special_tokens:\n",
    "        r2g_mapping.append(special_start_count)\n",
    "        special_start_count += 1\n",
    "    else:\n",
    "        g_idx = gpt_tokenizer.encoder[r_token]\n",
    "        r2g_mapping.append(g_idx)\n",
    "        \n",
    "# r2g = {v:k for k,v in enumerate(r2g_mapping)}\n",
    "# r2g_mapping = [r2g[i] for i in range(50265)]\n",
    "\n",
    "# rearrange the weight\n",
    "gpt2_model_states = gpt2_model.state_dict()\n",
    "gpt2_model_states = {k: v for k, v in gpt2_model_states.items() if '.attn.bias' not in k}\n",
    "gpt2_model_states['transformer.wte.weight'] = gpt2_model_states['transformer.wte.weight'][r2g_mapping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gpt2_model.state_dict(), \"unified_gpt2_large.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gpt2_model.half().state_dict(), \"unified_gpt2_large_fp16.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\")\n",
    "\n",
    "num_special_tokens = 8\n",
    "\n",
    "# copy the original embedding\n",
    "new_embedding = nn.Embedding(gpt2_model.config.vocab_size + num_special_tokens, gpt2_model.config.n_embd)\n",
    "new_embedding.weight.data[:gpt2_model.config.vocab_size, :] = gpt2_model.transformer.wte.weight.data\n",
    "\n",
    "# for the first three, use random interpolate\n",
    "for i in range(num_special_tokens):\n",
    "    new_embedding.weight.data[gpt2_model.config.vocab_size+i, :] = \\\n",
    "        random_interpolate(new_embedding.weight.data, gpt2_model.config.vocab_size)\n",
    "\n",
    "# set tied\n",
    "gpt2_model.transformer.wte = new_embedding\n",
    "gpt2_model.lm_head.weight = gpt2_model.transformer.wte.weight\n",
    "\n",
    "# redefine the mapping\n",
    "r2g_mapping = []\n",
    "special_start_count = 50257\n",
    "\n",
    "for r_idx in range(50265):\n",
    "    r_token = roberta_tokenizer.decoder[r_idx]\n",
    "    \n",
    "    if r_token in special_tokens:\n",
    "        r2g_mapping.append(special_start_count)\n",
    "        special_start_count += 1\n",
    "    else:\n",
    "        g_idx = gpt_tokenizer.encoder[r_token]\n",
    "        r2g_mapping.append(g_idx)\n",
    "        \n",
    "# r2g = {v:k for k,v in enumerate(r2g_mapping)}\n",
    "# r2g_mapping = [r2g[i] for i in range(50265)]\n",
    "\n",
    "# rearrange the weight\n",
    "gpt2_model_states = gpt2_model.state_dict()\n",
    "gpt2_model_states = {k: v for k, v in gpt2_model_states.items() if '.attn.bias' not in k}\n",
    "gpt2_model_states['transformer.wte.weight'] = gpt2_model_states['transformer.wte.weight'][r2g_mapping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gpt2_model.state_dict(), \"unified_gpt2_xl.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gpt2_model.half().state_dict(), \"unified_gpt2_xl_fp16.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distill GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 574/574 [00:00<00:00, 378364.06B/s]\n",
      "100%|██████████| 352833716/352833716 [00:30<00:00, 11435213.11B/s]\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "num_special_tokens = 8\n",
    "\n",
    "# copy the original embedding\n",
    "new_embedding = nn.Embedding(gpt2_model.config.vocab_size + num_special_tokens, gpt2_model.config.n_embd)\n",
    "new_embedding.weight.data[:gpt2_model.config.vocab_size, :] = gpt2_model.transformer.wte.weight.data\n",
    "\n",
    "# for the first three, use random interpolate\n",
    "for i in range(num_special_tokens):\n",
    "    new_embedding.weight.data[gpt2_model.config.vocab_size+i, :] = \\\n",
    "        random_interpolate(new_embedding.weight.data, gpt2_model.config.vocab_size)\n",
    "\n",
    "# set tied\n",
    "gpt2_model.transformer.wte = new_embedding\n",
    "gpt2_model.lm_head.weight = gpt2_model.transformer.wte.weight\n",
    "\n",
    "# redefine the mapping\n",
    "r2g_mapping = []\n",
    "special_start_count = 50257\n",
    "\n",
    "for r_idx in range(50265):\n",
    "    r_token = roberta_tokenizer.decoder[r_idx]\n",
    "    \n",
    "    if r_token in special_tokens:\n",
    "        r2g_mapping.append(special_start_count)\n",
    "        special_start_count += 1\n",
    "    else:\n",
    "        g_idx = gpt_tokenizer.encoder[r_token]\n",
    "        r2g_mapping.append(g_idx)\n",
    "        \n",
    "# r2g = {v:k for k,v in enumerate(r2g_mapping)}\n",
    "# r2g_mapping = [r2g[i] for i in range(50265)]\n",
    "\n",
    "# rearrange the weight\n",
    "gpt2_model_states = gpt2_model.state_dict()\n",
    "gpt2_model_states = {k: v for k, v in gpt2_model_states.items() if '.attn.bias' not in k}\n",
    "gpt2_model_states['transformer.wte.weight'] = gpt2_model_states['transformer.wte.weight'][r2g_mapping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gpt2_model.state_dict(), \"unified_gpt2_distill.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some random test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "481"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2g_mapping[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_tokenizer.decoder[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_tokenizer.decoder[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 588, 11875, 13]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_tokenizer.encode(\"I like cats.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final result\n",
    "# torch.save(new_gpt2_small.state_dict(), \"unified_gpt2_small.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_gpt2 = GPT2SimpleLM(UnifiedGPT2MediumConfig)\n",
    "new_gpt2.load_state_dict(gpt2_model_states, strict=False)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = new_gpt2\n",
    "model = model.to(device)\n",
    "\n",
    "# ask more about news\n",
    "prompt = roberta_tokenizer.encode(\"There is a cat. \")\n",
    "prompt = torch.LongTensor(prompt).to(device)\n",
    "\n",
    "prev_word = prompt.unsqueeze(0)\n",
    "past = None\n",
    "sentence = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for timestep in range(256):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits, past = model(prev_word, past=past)\n",
    "    logits = logits[:, -1, :]\n",
    "    logits = top_filtering(logits, top_p=0.9)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    prev_word = torch.multinomial(probs, num_samples=1)\n",
    "    #prev_word = torch.argmax(logits, -1).unsqueeze(1)\n",
    "    sentence.append(prev_word.item())\n",
    "    \n",
    "roberta_tokenizer.decode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer.encode(\"I like cats.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[r2g_mapping[item] for item in gpt_tokenizer.encode(\"I like cats.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_gpt2_small = GPT2SimpleLM(GPT2SmallConfig)\n",
    "new_gpt2_small.load_state_dict(gpt2_small.state_dict(), strict=False)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = new_gpt2_small\n",
    "model = model.to(device)\n",
    "\n",
    "# ask more about news\n",
    "prompt = gpt_tokenizer.encode(\"There is a cat\")\n",
    "prompt = torch.LongTensor(prompt).to(device)\n",
    "\n",
    "prev_word = prompt.unsqueeze(0)\n",
    "past = None\n",
    "sentence = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for timestep in range(256):\n",
    "    \n",
    "    logits, past = model(prev_word, past=past)\n",
    "    logits = logits[:, -1, :]\n",
    "    \n",
    "    #logits = top_filtering(logits, top_p=0.9)\n",
    "    #probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    #prev_word = torch.multinomial(probs, num_samples=1)\n",
    "    prev_word = torch.argmax(logits, -1).unsqueeze(1)\n",
    "    sentence.append(prev_word.item())\n",
    "    \n",
    "gpt_tokenizer.decode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
