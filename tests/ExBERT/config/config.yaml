training:
    num_gpus: 1 
    fp16: true
    fp16_opt_level: "O1" # O0 to disable fp16
    learning_rate: 1e-5
    gradient_accumulation_steps: 1
    max_grad_norm: 1.0 # disabled if negative
    optimizer: "AdamW"
    num_epoches: 1
exbert:
    gradient_checkpointing: false
    persistent_mem_size: 64
    num_attention_heads: 12
    num_hidden_layers: 12
    hidden_size: 768
    intermediate_size: 3072
    max_position_embeddings: 512
    attention_probs_dropout_prob: 0.0
    hidden_dropout_prob: 0.0
    embedding_std: 0.01
    layer_norm_eps: 1e-5
    hidden_act: "h_swish"
    # tokenizer
    vocab_size: 50265
    pad_token_id: 1